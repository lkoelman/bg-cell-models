{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print date and time of script execution\n",
    "import datetime\n",
    "print(\"\\nNotebook executed at at {} in following directory:\".format(datetime.datetime.now()))\n",
    "%cd /home/luye/workspace/bgcellmodels/GilliesWillshaw/\n",
    "\n",
    "# print code version (hash of checked out version)\n",
    "print(\"\\nCode version info:\")\n",
    "!git log -1 # --format=\"%H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "\n",
    "# Bokeh for interactive plots\n",
    "from bokeh.io import push_notebook, output_notebook, show as bokeh_show\n",
    "from bokeh.plotting import figure as bokeh_figure\n",
    "output_notebook()\n",
    "\n",
    "# Import our analysis modules\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport optimize.bpop_analysis_stn\n",
    "%aimport optimize.bpop_analysis_pop\n",
    "\n",
    "resp_analysis = optimize.bpop_analysis_stn\n",
    "pop_analysis = optimize.bpop_analysis_pop\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 individuals, 100 generations IBEA dataset\n",
    "checkpoint_files = [\n",
    "    ['IBEA_100gen',\n",
    "     '/home/luye/cloudstore_m/simdata/marasco_folding/optimization_run_20171103_2/opt_checkpoints_cdf893c2.pkl',\n",
    "     '/home/luye/cloudstore_m/simdata/marasco_folding/optimization_run_20171103_2/opt_checkpoints_cdf893c2_settings_withparams.pkl'],\n",
    "    ['NSGA2_100gen',\n",
    "     '/home/luye/cloudstore_m/simdata/marasco_folding/optimization_run_20171103_1/opt_checkpoints_3210b868.pkl',\n",
    "     '/home/luye/cloudstore_m/simdata/marasco_folding/optimization_run_20171103_1/opt_checkpoints_3210b868_settings.pkl']\n",
    "]\n",
    "\n",
    "opt_data = pd.DataFrame(checkpoint_files, columns=['name', 'checkpoints_file', 'settings_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimisation to analyse\n",
    "# opt_data['checkpoints_file'][0] # address by row index\n",
    "opt_name = 'IBEA_100gen'\n",
    "idx = opt_data.index[opt_data['name'] == opt_name][0] # addres by value in field\n",
    "\n",
    "cp_file = opt_data['checkpoints_file'][idx]\n",
    "settings_file = opt_data['settings_file'][idx]\n",
    "\n",
    "print(\"Analysing data from files:\\n{}\\n{}\".format(cp_file, settings_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logs file\n",
    "import cPickle as pickle\n",
    "\n",
    "# Old pickling method\n",
    "# with open(checkpoints_file, 'r') as f:\n",
    "#     checkpoint = pickle.load(f)\n",
    "#     # old_param_names = pickle.load(f)\n",
    "\n",
    "# New pickling method\n",
    "with open(cp_file, \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            checkpoint = pickle.load(f)\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "# Get variables\n",
    "hof = checkpoint['halloffame']\n",
    "log = checkpoint['logbook']\n",
    "pareto_front = checkpoint['paretofront']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load settings file\n",
    "with open(settings_file, 'r') as f:\n",
    "    opt_settings = pickle.load(f)\n",
    "    \n",
    "# pp.pprint(opt_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation protocol responses\n",
    "\n",
    "- make a random validation protocol\n",
    "- set up models and params like in opt notebook\n",
    "- plot all responses like in l5pc_analysis.py\n",
    "- calculate scores\n",
    "- select based on scores\n",
    "    + write to pickle file: selected params so we can instantiate and run bagged opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed logging\n",
    "from common import logutils\n",
    "\n",
    "# BluePyOpt\n",
    "import bluepyopt.ephys as ephys\n",
    "\n",
    "# Custom BluePyOpt modules\n",
    "from optimize.bpop_cellmodels import StnFullModel, StnReducedModel\n",
    "from optimize.bpop_protocols_stn import BpopProtocolWrapper\n",
    "import optimize.bpop_analysis_stn as resp_analysis\n",
    "import optimize.bpop_features_stn as features_stn\n",
    "\n",
    "# Physiology parameters\n",
    "from evalmodel.cellpopdata import StnModel\n",
    "from evalmodel.proto_common import StimProtocol\n",
    "\n",
    "CLAMP_PLATEAU = StimProtocol.CLAMP_PLATEAU\n",
    "CLAMP_REBOUND = StimProtocol.CLAMP_REBOUND\n",
    "MIN_SYN_BURST = StimProtocol.MIN_SYN_BURST\n",
    "SYN_BACKGROUND_HIGH = StimProtocol.SYN_BACKGROUND_HIGH\n",
    "\n",
    "# Adjust verbosity of loggers\n",
    "logutils.setLogLevel('quiet', ['marasco', 'folding', 'redops', 'stn_protos', 'bpop_ext',\n",
    "                               'bluepyopt.ephys.parameters', 'bluepyopt.ephys.simulators',\n",
    "                               'bluepyopt.ephys.efeatures', 'bluepyopt.ephys.recordings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose validation protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocols to use for optimisation\n",
    "validation_proto = SYN_BACKGROUND_HIGH\n",
    "\n",
    "# Collect al frozen mechanisms and parameters required for protocols to work\n",
    "proto_wrapper = BpopProtocolWrapper.make(validation_proto)\n",
    "proto_mechs, proto_params = BpopProtocolWrapper.all_mechs_params([proto_wrapper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run protocols using full model to get responses\n",
    "full_model = StnFullModel(\n",
    "                name\t\t= 'StnGillies',\n",
    "                mechs\t\t= proto_mechs,\n",
    "                params\t\t= proto_params)\n",
    "\n",
    "# Set up simulation\n",
    "nrnsim = ephys.simulators.NrnSimulator(dt=0.025, cvode_active=False)\n",
    "proto_wrapper.ephys_protocol.record_contained_traces = True\n",
    "\n",
    "full_responses = proto_wrapper.ephys_protocol.run(\n",
    "                            cell_model\t\t= full_model, \n",
    "                            param_values\t= {},\n",
    "                            sim\t\t\t\t= nrnsim,\n",
    "                            isolate\t\t\t= True)\n",
    "\n",
    "# Plot results\n",
    "# resp_analysis.plot_responses(full_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make EFEL feature objects\n",
    "stimprotos_feats = features_stn.make_opt_features([proto_wrapper])\n",
    "# returns: dict(StimProtocol : dict(feature_name : tuple(efeature, weight)))\n",
    "\n",
    "# Calculate target values from full model responses\n",
    "full_responses_dict = {proto_wrapper.ephys_protocol.name: full_responses}\n",
    "features_stn.calc_feature_targets(stimprotos_feats, full_responses_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make free parameters (locations etc, not values)\n",
    "opt_param_names = opt_settings['opt_param_names'] # same order as in individuals\n",
    "opt_params = opt_settings['free_params'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Create reduced model and get parameters\n",
    "red_model = StnReducedModel(\n",
    "                name\t\t= 'StnFolded',\n",
    "                fold_method\t= 'marasco',\n",
    "                num_passes\t= 7,\n",
    "                mechs\t\t= proto_mechs,\n",
    "                params\t\t= proto_params + opt_params)\n",
    "\n",
    "all_ind_responses = []\n",
    "all_ind_scores = []\n",
    "\n",
    "for ind in hof:\n",
    "    \n",
    "    ind_param_dict = {pname: ind[i] for i,pname in enumerate(opt_param_names)}\n",
    "    \n",
    "    # Run with individual's parameters\n",
    "    ind_responses = proto_wrapper.ephys_protocol.run(\n",
    "                            cell_model\t\t= red_model, \n",
    "                            param_values\t= ind_param_dict,\n",
    "                            sim\t\t\t\t= nrnsim,\n",
    "                            isolate\t\t\t= True)\n",
    "    \n",
    "    all_ind_responses.append(ind_responses)\n",
    "    # resp_analysis.plot_responses(ind_responses)\n",
    "                         \n",
    "    # Calculate feature scores\n",
    "    # (iterate over dict(StimProtocol : dict(feature_name : tuple(efeature, weight))))\n",
    "    all_ind_scores.append({})\n",
    "    for stimproto, featdict in stimprotos_feats.iteritems():\n",
    "        for efeat, weight in featdict.values():\n",
    "\n",
    "            # NOTE: score = distance = sum(feat[i] - exp_mean) / N / exp_std  => so exp_std determines weight\n",
    "            score = efeat.calculate_score(ind_responses)\n",
    "            dist = score * efeat.exp_std\n",
    "            \n",
    "            all_ind_scores[-1][efeat.name] = dist\n",
    "            print(\"Score (unweighted) for {} is {}\".format(efeat.name, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare voltage responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_responses(response_dicts):\n",
    "    \"\"\"\n",
    "    Plot response dict for each individual\n",
    "\n",
    "    @param response_dicts    list(dict<str, TimeVoltageResponse>)\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(len(response_dicts) * len(response_dicts[0]))\n",
    "    try:\n",
    "        iter(axes)\n",
    "    except TypeError:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i_ind, responses in enumerate(response_dicts):\n",
    "        for i_resp, (resp_name, response) in enumerate(sorted(responses.items())):\n",
    "            axes[i_ind+i_resp].plot(response['time'], response['voltage'], label=resp_name)\n",
    "            # axes[i_ind+i_resp].set_title(resp_name)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = plt.figure()\n",
    "response = full_responses.items()[0][1]\n",
    "l1 = plt.plot(response['time'], response['voltage'], color='g')\n",
    "\n",
    "# Plot individual responses\n",
    "f2, a2 = plot_responses(all_ind_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare raster plots & PSTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spike times\n",
    "import efel\n",
    "efel.reset()\n",
    "efel.setThreshold(-20.0) # eFEL default value\n",
    "\n",
    "def get_peaktimes(tvresp, proto):\n",
    "    \"\"\"\n",
    "    Function to extract peak times from TimeVoltageResponse\n",
    "    \"\"\"\n",
    "    # Prepare trace\n",
    "    efel_trace = {\n",
    "        'T': tvresp['time'],\n",
    "        'V': tvresp['voltage'],\n",
    "        'stim_start': [proto.response_interval[0]],\n",
    "        'stim_end': [proto.response_interval[1]],\n",
    "    }\n",
    "\n",
    "    # Calculate spike times from response\n",
    "    values = efel.getFeatureValues([efel_trace], ['peak_time'], raise_warnings=True)\n",
    "    return values[0]['peak_time']\n",
    "    \n",
    "\n",
    "# Get spike times for all individuals\n",
    "all_ind_spiketimes = []\n",
    "for i, responses in enumerate(all_ind_responses):\n",
    "    peak_times = get_peaktimes(responses.items()[0][1], proto_wrapper)\n",
    "    all_ind_spiketimes.append(peak_times)\n",
    "    \n",
    "full_spiketimes = get_peaktimes(full_responses.items()[0][1], proto_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import analysis\n",
    "import collections\n",
    "\n",
    "all_spk = collections.OrderedDict()\n",
    "# all_spk['orig'] = full_spiketimes\n",
    "for i,st in enumerate(all_ind_spiketimes):\n",
    "    all_spk['ind{}'.format(i)] = st\n",
    "\n",
    "# fig, ax = analysis.plotRaster(all_spk, proto_wrapper.response_interval)\n",
    "# ax.set_yticklabels(reversed(all_spk.keys()))\n",
    "# fig.set_figheight(0.2*len(all_spk))\n",
    "\n",
    "# create X and Y data for scatter plot\n",
    "spike_labels = list(reversed(all_spk.keys()))\n",
    "spike_vecs = [all_spk[label] for label in spike_labels]\n",
    "x_data = np.concatenate(spike_vecs) # X data is concatenated spike times\n",
    "y_data = np.concatenate([np.zeros_like(vec)+j for j, vec in enumerate(spike_vecs)]) # Y-data is trace IDs\n",
    "\n",
    "# Filter data within given time interval\n",
    "timeRange = proto_wrapper.response_interval\n",
    "mask = (x_data > timeRange[0]) & (x_data < timeRange[1])\n",
    "x_data = x_data[mask]\n",
    "y_data = y_data[mask]\n",
    "\n",
    "# Plot data as scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_data, y_data, s=4, c='b', lw=0, marker='.') # marker=',' is thicker\n",
    "ax.plot(full_spiketimes, np.zeros_like(full_spiketimes)+len(spike_vecs), c='r', lw=0, markersize=2, marker='.')\n",
    "\n",
    "# Axes\n",
    "ax.set_xlim(timeRange)\n",
    "ax.grid(True, axis='x')\n",
    "\n",
    "plt.yticks(range(len(spike_labels)), spike_labels, rotation='horizontal')\n",
    "ax.set_xlabel('time (ms)')\n",
    "ax.set_title('Best individual spike times', loc='center')\n",
    "\n",
    "fig.subplots_adjust(left=0.15) # Tweak spacing to prevent clipping of tick-labels\n",
    "fig.set_figheight(0.2*len(all_spk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "886px",
    "left": "0px",
    "right": "1431px",
    "top": "67px",
    "width": "249px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
